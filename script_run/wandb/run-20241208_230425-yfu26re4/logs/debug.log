2024-12-08 23:04:25,374 INFO    MainThread:3099 [wandb_setup.py:_flush():68] Current SDK version is 0.19.0
2024-12-08 23:04:25,374 INFO    MainThread:3099 [wandb_setup.py:_flush():68] Configure stats pid to 3099
2024-12-08 23:04:25,374 INFO    MainThread:3099 [wandb_setup.py:_flush():68] Loading settings from /root/.config/wandb/settings
2024-12-08 23:04:25,374 INFO    MainThread:3099 [wandb_setup.py:_flush():68] Loading settings from /home/CATDF-Diffusion/script_run/wandb/settings
2024-12-08 23:04:25,374 INFO    MainThread:3099 [wandb_setup.py:_flush():68] Loading settings from environment variables
2024-12-08 23:04:25,375 INFO    MainThread:3099 [wandb_init.py:_log_setup():528] Logging user logs to /home/CATDF-Diffusion/script_run/wandb/run-20241208_230425-yfu26re4/logs/debug.log
2024-12-08 23:04:25,375 INFO    MainThread:3099 [wandb_init.py:_log_setup():529] Logging internal logs to /home/CATDF-Diffusion/script_run/wandb/run-20241208_230425-yfu26re4/logs/debug-internal.log
2024-12-08 23:04:25,375 INFO    MainThread:3099 [wandb_init.py:init():639] calling init triggers
2024-12-08 23:04:25,375 INFO    MainThread:3099 [wandb_init.py:init():645] wandb.init called with sweep_config: {}
config: {}
2024-12-08 23:04:25,375 INFO    MainThread:3099 [wandb_init.py:init():688] starting backend
2024-12-08 23:04:25,375 INFO    MainThread:3099 [wandb_init.py:init():692] sending inform_init request
2024-12-08 23:04:25,384 INFO    MainThread:3099 [backend.py:_multiprocessing_setup():104] multiprocessing start_methods=fork,spawn,forkserver, using: spawn
2024-12-08 23:04:25,385 INFO    MainThread:3099 [wandb_init.py:init():705] backend started and connected
2024-12-08 23:04:25,387 INFO    MainThread:3099 [wandb_init.py:init():798] updated telemetry
2024-12-08 23:04:25,401 INFO    MainThread:3099 [wandb_init.py:init():830] communicating run to backend with 90.0 second timeout
2024-12-08 23:04:25,802 INFO    MainThread:3099 [wandb_init.py:init():882] starting run threads in backend
2024-12-08 23:04:25,987 INFO    MainThread:3099 [wandb_run.py:_console_start():2443] atexit reg
2024-12-08 23:04:25,987 INFO    MainThread:3099 [wandb_run.py:_redirect():2293] redirect: wrap_raw
2024-12-08 23:04:25,987 INFO    MainThread:3099 [wandb_run.py:_redirect():2358] Wrapping output streams.
2024-12-08 23:04:25,987 INFO    MainThread:3099 [wandb_run.py:_redirect():2383] Redirects installed.
2024-12-08 23:04:25,990 INFO    MainThread:3099 [wandb_init.py:init():925] run started, returning control to user process
2024-12-08 23:04:25,990 INFO    MainThread:3099 [wandb_run.py:_config_callback():1375] config_cb None None {'learning_rate': 1e-05, 'train_batch_size': 4, 'num_train_epochs': 200, 'resolution': 512}
2024-12-08 23:04:47,930 INFO    MainThread:3099 [wandb_run.py:_config_callback():1375] config_cb None None {'unet': None, 'device_train': 'cuda', 'noise_scheduler': 'DDPMScheduler {\n  "_class_name": "DDPMScheduler",\n  "_diffusers_version": "0.31.0",\n  "beta_end": 0.012,\n  "beta_schedule": "scaled_linear",\n  "beta_start": 0.00085,\n  "clip_sample": false,\n  "clip_sample_range": 1.0,\n  "dynamic_thresholding_ratio": 0.995,\n  "num_train_timesteps": 1000,\n  "prediction_type": "sample",\n  "rescale_betas_zero_snr": false,\n  "sample_max_value": 1.0,\n  "set_alpha_to_one": false,\n  "skip_prk_steps": true,\n  "steps_offset": 1,\n  "thresholding": false,\n  "timestep_spacing": "leading",\n  "trained_betas": null,\n  "variance_type": "fixed_small"\n}\n', 'scheduler_optimizer_config': 'TrainableParameters(unet=UNet2DConditionModel(\n  (conv_in): Conv2d(9, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (time_proj): Timesteps()\n  (time_embedding): TimestepEmbedding(\n    (linear_1): Linear(in_features=320, out_features=1280, bias=True)\n    (act): SiLU()\n    (linear_2): Linear(in_features=1280, out_features=1280, bias=True)\n  )\n  (down_blocks): ModuleList(\n    (0): CrossAttnDownBlock2D(\n      (attentions): ModuleList(\n        (0-1): 2 x Transformer2DModel(\n          (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n          (proj_in): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n          (transformer_blocks): ModuleList(\n            (0): BasicTransformerBlock(\n              (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n              (attn1): Attention(\n                (to_q): Linear(in_features=320, out_features=320, bias=False)\n                (to_k): Linear(in_features=320, out_features=320, bias=False)\n                (to_v): Linear(in_features=320, out_features=320, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=320, out_features=320, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n                (processor): AttnProcessor2_0()\n              )\n              (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n              (attn2): Attention(\n                (to_q): Linear(in_features=320, out_features=320, bias=False)\n                (to_k): Linear(in_features=768, out_features=320, bias=False)\n                (to_v): Linear(in_features=768, out_features=320, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=320, out_features=320, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n                (processor): SkipAttnProcessor()\n              )\n              (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n              (ff): FeedForward(\n                (net): ModuleList(\n                  (0): GEGLU(\n                    (proj): Linear(in_features=320, out_features=2560, bias=True)\n                  )\n                  (1): Dropout(p=0.0, inplace=False)\n                  (2): Linear(in_features=1280, out_features=320, bias=True)\n                )\n              )\n            )\n          )\n          (proj_out): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (resnets): ModuleList(\n        (0-1): 2 x ResnetBlock2D(\n          (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n          (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n        )\n      )\n      (downsamplers): ModuleList(\n        (0): Downsample2D(\n          (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        )\n      )\n    )\n    (1): CrossAttnDownBlock2D(\n      (attentions): ModuleList(\n        (0-1): 2 x Transformer2DModel(\n          (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n          (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n          (transformer_blocks): ModuleList(\n            (0): BasicTransformerBlock(\n              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n              (attn1): Attention(\n                (to_q): Linear(in_features=640, out_features=640, bias=False)\n                (to_k): Linear(in_features=640, out_features=640, bias=False)\n                (to_v): Linear(in_features=640, out_features=640, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=640, out_features=640, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n                (processor): AttnProcessor2_0()\n              )\n              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n              (attn2): Attention(\n                (to_q): Linear(in_features=640, out_features=640, bias=False)\n                (to_k): Linear(in_features=768, out_features=640, bias=False)\n                (to_v): Linear(in_features=768, out_features=640, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=640, out_features=640, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n                (processor): SkipAttnProcessor()\n              )\n              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n              (ff): FeedForward(\n                (net): ModuleList(\n                  (0): GEGLU(\n                    (proj): Linear(in_features=640, out_features=5120, bias=True)\n                  )\n                  (1): Dropout(p=0.0, inplace=False)\n                  (2): Linear(in_features=2560, out_features=640, bias=True)\n                )\n              )\n            )\n          )\n          (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (resnets): ModuleList(\n        (0): ResnetBlock2D(\n          (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n          (conv1): Conv2d(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): Conv2d(320, 640, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (1): ResnetBlock2D(\n          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n          (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n        )\n      )\n      (downsamplers): ModuleList(\n        (0): Downsample2D(\n          (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        )\n      )\n    )\n    (2): CrossAttnDownBlock2D(\n      (attentions): ModuleList(\n        (0-1): 2 x Transformer2DModel(\n          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n          (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n          (transformer_blocks): ModuleList(\n            (0): BasicTransformerBlock(\n              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n              (attn1): Attention(\n                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n                (processor): AttnProcessor2_0()\n              )\n              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n              (attn2): Attention(\n                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n                (to_k): Linear(in_features=768, out_features=1280, bias=False)\n                (to_v): Linear(in_features=768, out_features=1280, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n                (processor): SkipAttnProcessor()\n              )\n              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n              (ff): FeedForward(\n                (net): ModuleList(\n                  (0): GEGLU(\n                    (proj): Linear(in_features=1280, out_features=10240, bias=True)\n                  )\n                  (1): Dropout(p=0.0, inplace=False)\n                  (2): Linear(in_features=5120, out_features=1280, bias=True)\n                )\n              )\n            )\n          )\n          (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (resnets): ModuleList(\n        (0): ResnetBlock2D(\n          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n          (conv1): Conv2d(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): Conv2d(640, 1280, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (1): ResnetBlock2D(\n          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n        )\n      )\n      (downsamplers): ModuleList(\n        (0): Downsample2D(\n          (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        )\n      )\n    )\n    (3): DownBlock2D(\n      (resnets): ModuleList(\n        (0-1): 2 x ResnetBlock2D(\n          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n        )\n      )\n    )\n  )\n  (up_blocks): ModuleList(\n    (0): UpBlock2D(\n      (resnets): ModuleList(\n        (0-2): 3 x ResnetBlock2D(\n          (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n          (conv1): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (upsamplers): ModuleList(\n        (0): Upsample2D(\n          (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        )\n      )\n    )\n    (1): CrossAttnUpBlock2D(\n      (attentions): ModuleList(\n        (0-2): 3 x Transformer2DModel(\n          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n          (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n          (transformer_blocks): ModuleList(\n            (0): BasicTransformerBlock(\n              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n              (attn1): Attention(\n                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n                (processor): AttnProcessor2_0()\n              )\n              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n              (attn2): Attention(\n                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n                (to_k): Linear(in_features=768, out_features=1280, bias=False)\n                (to_v): Linear(in_features=768, out_features=1280, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n                (processor): SkipAttnProcessor()\n              )\n              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n              (ff): FeedForward(\n                (net): ModuleList(\n                  (0): GEGLU(\n                    (proj): Linear(in_features=1280, out_features=10240, bias=True)\n                  )\n                  (1): Dropout(p=0.0, inplace=False)\n                  (2): Linear(in_features=5120, out_features=1280, bias=True)\n                )\n              )\n            )\n          )\n          (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (resnets): ModuleList(\n        (0-1): 2 x ResnetBlock2D(\n          (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n          (conv1): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (2): ResnetBlock2D(\n          (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n          (conv1): Conv2d(1920, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): Conv2d(1920, 1280, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (upsamplers): ModuleList(\n        (0): Upsample2D(\n          (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        )\n      )\n    )\n    (2): CrossAttnUpBlock2D(\n      (attentions): ModuleList(\n        (0-2): 3 x Transformer2DModel(\n          (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n          (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n          (transformer_blocks): ModuleList(\n            (0): BasicTransformerBlock(\n              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n              (attn1): Attention(\n                (to_q): Linear(in_features=640, out_features=640, bias=False)\n                (to_k): Linear(in_features=640, out_features=640, bias=False)\n                (to_v): Linear(in_features=640, out_features=640, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=640, out_features=640, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n                (processor): AttnProcessor2_0()\n              )\n              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n              (attn2): Attention(\n                (to_q): Linear(in_features=640, out_features=640, bias=False)\n                (to_k): Linear(in_features=768, out_features=640, bias=False)\n                (to_v): Linear(in_features=768, out_features=640, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=640, out_features=640, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n                (processor): SkipAttnProcessor()\n              )\n              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n              (ff): FeedForward(\n                (net): ModuleList(\n                  (0): GEGLU(\n                    (proj): Linear(in_features=640, out_features=5120, bias=True)\n                  )\n                  (1): Dropout(p=0.0, inplace=False)\n                  (2): Linear(in_features=2560, out_features=640, bias=True)\n                )\n              )\n            )\n          )\n          (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (resnets): ModuleList(\n        (0): ResnetBlock2D(\n          (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n          (conv1): Conv2d(1920, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (1): ResnetBlock2D(\n          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          (conv1): Conv2d(1280, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (2): ResnetBlock2D(\n          (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n          (conv1): Conv2d(960, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): Conv2d(960, 640, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (upsamplers): ModuleList(\n        (0): Upsample2D(\n          (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        )\n      )\n    )\n    (3): CrossAttnUpBlock2D(\n      (attentions): ModuleList(\n        (0-2): 3 x Transformer2DModel(\n          (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n          (proj_in): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n          (transformer_blocks): ModuleList(\n            (0): BasicTransformerBlock(\n              (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n              (attn1): Attention(\n                (to_q): Linear(in_features=320, out_features=320, bias=False)\n                (to_k): Linear(in_features=320, out_features=320, bias=False)\n                (to_v): Linear(in_features=320, out_features=320, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=320, out_features=320, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n                (processor): AttnProcessor2_0()\n              )\n              (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n              (attn2): Attention(\n                (to_q): Linear(in_features=320, out_features=320, bias=False)\n                (to_k): Linear(in_features=768, out_features=320, bias=False)\n                (to_v): Linear(in_features=768, out_features=320, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=320, out_features=320, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n                (processor): SkipAttnProcessor()\n              )\n              (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n              (ff): FeedForward(\n                (net): ModuleList(\n                  (0): GEGLU(\n                    (proj): Linear(in_features=320, out_features=2560, bias=True)\n                  )\n                  (1): Dropout(p=0.0, inplace=False)\n                  (2): Linear(in_features=1280, out_features=320, bias=True)\n                )\n              )\n            )\n          )\n          (proj_out): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (resnets): ModuleList(\n        (0): ResnetBlock2D(\n          (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n          (conv1): Conv2d(960, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (1-2): 2 x ResnetBlock2D(\n          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n          (conv1): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n    )\n  )\n  (mid_block): UNetMidBlock2DCrossAttn(\n    (attentions): ModuleList(\n      (0): Transformer2DModel(\n        (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n        (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n        (transformer_blocks): ModuleList(\n          (0): BasicTransformerBlock(\n            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n            (attn1): Attention(\n              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n              (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n              (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n              (to_out): ModuleList(\n                (0): Linear(in_features=1280, out_features=1280, bias=True)\n                (1): Dropout(p=0.0, inplace=False)\n              )\n              (processor): AttnProcessor2_0()\n            )\n            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n            (attn2): Attention(\n              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n              (to_k): Linear(in_features=768, out_features=1280, bias=False)\n              (to_v): Linear(in_features=768, out_features=1280, bias=False)\n              (to_out): ModuleList(\n                (0): Linear(in_features=1280, out_features=1280, bias=True)\n                (1): Dropout(p=0.0, inplace=False)\n              )\n              (processor): SkipAttnProcessor()\n            )\n            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n            (ff): FeedForward(\n              (net): ModuleList(\n                (0): GEGLU(\n                  (proj): Linear(in_features=1280, out_features=10240, bias=True)\n                )\n                (1): Dropout(p=0.0, inplace=False)\n                (2): Linear(in_features=5120, out_features=1280, bias=True)\n              )\n            )\n          )\n        )\n        (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n      )\n    )\n    (resnets): ModuleList(\n      (0-1): 2 x ResnetBlock2D(\n        (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n        (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n        (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (nonlinearity): SiLU()\n      )\n    )\n  )\n  (conv_norm_out): GroupNorm(32, 320, eps=1e-05, affine=True)\n  (conv_act): SiLU()\n  (conv_out): Conv2d(320, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n), learning_rate=1e-05)', 'args': "Namespace(pretrained_model_name_or_path='botp/stable-diffusion-v1-5-inpainting', unet_model_name_or_path='/home/weight_pretrain/unet', tokenizer_name=None, load_unet_increaments=None, dataset_path='/home/data/data_high_quality.csv', save_embeddings_to_npz=False, revision=None, variant=None, output_dir='/home/save_checkpoint', seed=None, resolution=512, train_batch_size=4, num_train_epochs=200, max_train_steps=None, checkpointing_steps=500, checkpoints_total_limit=5, resume_from_checkpoint='latest', input_type='raw', gradient_accumulation_steps=1, gradient_checkpointing=False, learning_rate=1e-05, tensor_dtype_save='fp16', path_to_save_data_embedding='/home/data/embedding', scale_lr=False, lr_scheduler='constant', prediction_type='sample', lr_warmup_steps=500, lr_num_cycles=1, lr_power=1.0, dataloader_num_workers=0, adam_beta1=0.9, adam_beta2=0.999, adam_weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, logging_dir='logs', allow_tf32=False, report_to='wandb', mixed_precision='bf16', enable_xformers_memory_efficient_attention=False, set_grads_to_none=False, max_train_samples=None, proportion_empty_prompts=0, tracker_project_name='train_controlnext')"}
2024-12-08 23:23:11,959 WARNING MsgRouterThr:3099 [router.py:message_loop():75] message_loop has been closed
