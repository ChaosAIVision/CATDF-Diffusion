_wandb:
    value:
        cli_version: 0.19.0
        m: []
        python_version: 3.10.16
        t:
            "1":
                - 1
                - 11
                - 41
                - 49
                - 51
                - 55
                - 71
                - 83
                - 106
            "2":
                - 1
                - 11
                - 41
                - 49
                - 51
                - 55
                - 71
                - 83
                - 106
            "3":
                - 23
                - 55
            "4": 3.10.16
            "5": 0.19.0
            "6": 4.47.0
            "8":
                - 5
            "12": 0.19.0
            "13": linux-x86_64
args:
    value: Namespace(pretrained_model_name_or_path='botp/stable-diffusion-v1-5-inpainting', unet_model_name_or_path='/home/weight_pretrain/unet', tokenizer_name=None, load_unet_increaments=None, dataset_path='/home/data/data_high_quality.csv', save_embeddings_to_npz=False, revision=None, variant=None, output_dir='/home/save_checkpoint', seed=None, resolution=512, train_batch_size=6, num_train_epochs=100, max_train_steps=None, checkpointing_steps=500, checkpoints_total_limit=5, resume_from_checkpoint='latest', input_type='raw', gradient_accumulation_steps=2, gradient_checkpointing=False, learning_rate=0.0001, tensor_dtype_save='fp16', path_to_save_data_embedding='/home/data/embedding', scale_lr=False, lr_scheduler='constant', prediction_type='sample', lr_warmup_steps=500, lr_num_cycles=1, lr_power=1.0, dataloader_num_workers=0, adam_beta1=0.9, adam_beta2=0.999, adam_weight_decay=0.01, adam_epsilon=1e-08, max_grad_norm=1.0, logging_dir='logs', allow_tf32=False, report_to='wandb', mixed_precision='bf16', enable_xformers_memory_efficient_attention=False, set_grads_to_none=False, max_train_samples=None, proportion_empty_prompts=0, tracker_project_name='train_controlnext')
device_train:
    value: cuda
learning_rate:
    value: 0.0001
noise_scheduler:
    value: |
        DDPMScheduler {
          "_class_name": "DDPMScheduler",
          "_diffusers_version": "0.31.0",
          "beta_end": 0.012,
          "beta_schedule": "scaled_linear",
          "beta_start": 0.00085,
          "clip_sample": false,
          "clip_sample_range": 1.0,
          "dynamic_thresholding_ratio": 0.995,
          "num_train_timesteps": 1000,
          "prediction_type": "sample",
          "rescale_betas_zero_snr": false,
          "sample_max_value": 1.0,
          "set_alpha_to_one": false,
          "skip_prk_steps": true,
          "steps_offset": 1,
          "thresholding": false,
          "timestep_spacing": "leading",
          "trained_betas": null,
          "variance_type": "fixed_small"
        }
num_train_epochs:
    value: 100
resolution:
    value: 512
scheduler_optimizer_config:
    value: |-
        TrainableParameters(unet=UNet2DConditionModel(
          (conv_in): Conv2d(9, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (time_proj): Timesteps()
          (time_embedding): TimestepEmbedding(
            (linear_1): Linear(in_features=320, out_features=1280, bias=True)
            (act): SiLU()
            (linear_2): Linear(in_features=1280, out_features=1280, bias=True)
          )
          (down_blocks): ModuleList(
            (0): CrossAttnDownBlock2D(
              (attentions): ModuleList(
                (0-1): 2 x Transformer2DModel(
                  (norm): GroupNorm(32, 320, eps=1e-06, affine=True)
                  (proj_in): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
                  (transformer_blocks): ModuleList(
                    (0): BasicTransformerBlock(
                      (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                      (attn1): Attention(
                        (to_q): Linear(in_features=320, out_features=320, bias=False)
                        (to_k): Linear(in_features=320, out_features=320, bias=False)
                        (to_v): Linear(in_features=320, out_features=320, bias=False)
                        (to_out): ModuleList(
                          (0): Linear(in_features=320, out_features=320, bias=True)
                          (1): Dropout(p=0.0, inplace=False)
                        )
                        (processor): AttnProcessor2_0()
                      )
                      (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                      (attn2): Attention(
                        (to_q): Linear(in_features=320, out_features=320, bias=False)
                        (to_k): Linear(in_features=768, out_features=320, bias=False)
                        (to_v): Linear(in_features=768, out_features=320, bias=False)
                        (to_out): ModuleList(
                          (0): Linear(in_features=320, out_features=320, bias=True)
                          (1): Dropout(p=0.0, inplace=False)
                        )
                        (processor): SkipAttnProcessor()
                      )
                      (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                      (ff): FeedForward(
                        (net): ModuleList(
                          (0): GEGLU(
                            (proj): Linear(in_features=320, out_features=2560, bias=True)
                          )
                          (1): Dropout(p=0.0, inplace=False)
                          (2): Linear(in_features=1280, out_features=320, bias=True)
                        )
                      )
                    )
                  )
                  (proj_out): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
                )
              )
              (resnets): ModuleList(
                (0-1): 2 x ResnetBlock2D(
                  (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)
                  (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)
                  (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)
                  (dropout): Dropout(p=0.0, inplace=False)
                  (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  (nonlinearity): SiLU()
                )
              )
              (downsamplers): ModuleList(
                (0): Downsample2D(
                  (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                )
              )
            )
            (1): CrossAttnDownBlock2D(
              (attentions): ModuleList(
                (0-1): 2 x Transformer2DModel(
                  (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
                  (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))
                  (transformer_blocks): ModuleList(
                    (0): BasicTransformerBlock(
                      (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                      (attn1): Attention(
                        (to_q): Linear(in_features=640, out_features=640, bias=False)
                        (to_k): Linear(in_features=640, out_features=640, bias=False)
                        (to_v): Linear(in_features=640, out_features=640, bias=False)
                        (to_out): ModuleList(
                          (0): Linear(in_features=640, out_features=640, bias=True)
                          (1): Dropout(p=0.0, inplace=False)
                        )
                        (processor): AttnProcessor2_0()
                      )
                      (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                      (attn2): Attention(
                        (to_q): Linear(in_features=640, out_features=640, bias=False)
                        (to_k): Linear(in_features=768, out_features=640, bias=False)
                        (to_v): Linear(in_features=768, out_features=640, bias=False)
                        (to_out): ModuleList(
                          (0): Linear(in_features=640, out_features=640, bias=True)
                          (1): Dropout(p=0.0, inplace=False)
                        )
                        (processor): SkipAttnProcessor()
                      )
                      (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                      (ff): FeedForward(
                        (net): ModuleList(
                          (0): GEGLU(
                            (proj): Linear(in_features=640, out_features=5120, bias=True)
                          )
                          (1): Dropout(p=0.0, inplace=False)
                          (2): Linear(in_features=2560, out_features=640, bias=True)
                        )
                      )
                    )
                  )
                  (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))
                )
              )
              (resnets): ModuleList(
                (0): ResnetBlock2D(
                  (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)
                  (conv1): Conv2d(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)
                  (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)
                  (dropout): Dropout(p=0.0, inplace=False)
                  (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  (nonlinearity): SiLU()
                  (conv_shortcut): Conv2d(320, 640, kernel_size=(1, 1), stride=(1, 1))
                )
                (1): ResnetBlock2D(
                  (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)
                  (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)
                  (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)
                  (dropout): Dropout(p=0.0, inplace=False)
                  (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  (nonlinearity): SiLU()
                )
              )
              (downsamplers): ModuleList(
                (0): Downsample2D(
                  (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                )
              )
            )
            (2): CrossAttnDownBlock2D(
              (attentions): ModuleList(
                (0-1): 2 x Transformer2DModel(
                  (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
                  (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))
                  (transformer_blocks): ModuleList(
                    (0): BasicTransformerBlock(
                      (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                      (attn1): Attention(
                        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                        (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                        (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                        (to_out): ModuleList(
                          (0): Linear(in_features=1280, out_features=1280, bias=True)
                          (1): Dropout(p=0.0, inplace=False)
                        )
                        (processor): AttnProcessor2_0()
                      )
                      (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                      (attn2): Attention(
                        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                        (to_k): Linear(in_features=768, out_features=1280, bias=False)
                        (to_v): Linear(in_features=768, out_features=1280, bias=False)
                        (to_out): ModuleList(
                          (0): Linear(in_features=1280, out_features=1280, bias=True)
                          (1): Dropout(p=0.0, inplace=False)
                        )
                        (processor): SkipAttnProcessor()
                      )
                      (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                      (ff): FeedForward(
                        (net): ModuleList(
                          (0): GEGLU(
                            (proj): Linear(in_features=1280, out_features=10240, bias=True)
                          )
                          (1): Dropout(p=0.0, inplace=False)
                          (2): Linear(in_features=5120, out_features=1280, bias=True)
                        )
                      )
                    )
                  )
                  (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))
                )
              )
              (resnets): ModuleList(
                (0): ResnetBlock2D(
                  (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)
                  (conv1): Conv2d(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)
                  (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)
                  (dropout): Dropout(p=0.0, inplace=False)
                  (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  (nonlinearity): SiLU()
                  (conv_shortcut): Conv2d(640, 1280, kernel_size=(1, 1), stride=(1, 1))
                )
                (1): ResnetBlock2D(
                  (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)
                  (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)
                  (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)
                  (dropout): Dropout(p=0.0, inplace=False)
                  (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  (nonlinearity): SiLU()
                )
              )
              (downsamplers): ModuleList(
                (0): Downsample2D(
                  (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                )
              )
            )
            (3): DownBlock2D(
              (resnets): ModuleList(
                (0-1): 2 x ResnetBlock2D(
                  (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)
                  (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)
                  (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)
                  (dropout): Dropout(p=0.0, inplace=False)
                  (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  (nonlinearity): SiLU()
                )
              )
            )
          )
          (up_blocks): ModuleList(
            (0): UpBlock2D(
              (resnets): ModuleList(
                (0-2): 3 x ResnetBlock2D(
                  (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)
                  (conv1): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)
                  (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)
                  (dropout): Dropout(p=0.0, inplace=False)
                  (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  (nonlinearity): SiLU()
                  (conv_shortcut): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))
                )
              )
              (upsamplers): ModuleList(
                (0): Upsample2D(
                  (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                )
              )
            )
            (1): CrossAttnUpBlock2D(
              (attentions): ModuleList(
                (0-2): 3 x Transformer2DModel(
                  (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
                  (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))
                  (transformer_blocks): ModuleList(
                    (0): BasicTransformerBlock(
                      (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                      (attn1): Attention(
                        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                        (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                        (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                        (to_out): ModuleList(
                          (0): Linear(in_features=1280, out_features=1280, bias=True)
                          (1): Dropout(p=0.0, inplace=False)
                        )
                        (processor): AttnProcessor2_0()
                      )
                      (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                      (attn2): Attention(
                        (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                        (to_k): Linear(in_features=768, out_features=1280, bias=False)
                        (to_v): Linear(in_features=768, out_features=1280, bias=False)
                        (to_out): ModuleList(
                          (0): Linear(in_features=1280, out_features=1280, bias=True)
                          (1): Dropout(p=0.0, inplace=False)
                        )
                        (processor): SkipAttnProcessor()
                      )
                      (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                      (ff): FeedForward(
                        (net): ModuleList(
                          (0): GEGLU(
                            (proj): Linear(in_features=1280, out_features=10240, bias=True)
                          )
                          (1): Dropout(p=0.0, inplace=False)
                          (2): Linear(in_features=5120, out_features=1280, bias=True)
                        )
                      )
                    )
                  )
                  (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))
                )
              )
              (resnets): ModuleList(
                (0-1): 2 x ResnetBlock2D(
                  (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)
                  (conv1): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)
                  (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)
                  (dropout): Dropout(p=0.0, inplace=False)
                  (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  (nonlinearity): SiLU()
                  (conv_shortcut): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))
                )
                (2): ResnetBlock2D(
                  (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)
                  (conv1): Conv2d(1920, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)
                  (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)
                  (dropout): Dropout(p=0.0, inplace=False)
                  (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  (nonlinearity): SiLU()
                  (conv_shortcut): Conv2d(1920, 1280, kernel_size=(1, 1), stride=(1, 1))
                )
              )
              (upsamplers): ModuleList(
                (0): Upsample2D(
                  (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                )
              )
            )
            (2): CrossAttnUpBlock2D(
              (attentions): ModuleList(
                (0-2): 3 x Transformer2DModel(
                  (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
                  (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))
                  (transformer_blocks): ModuleList(
                    (0): BasicTransformerBlock(
                      (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                      (attn1): Attention(
                        (to_q): Linear(in_features=640, out_features=640, bias=False)
                        (to_k): Linear(in_features=640, out_features=640, bias=False)
                        (to_v): Linear(in_features=640, out_features=640, bias=False)
                        (to_out): ModuleList(
                          (0): Linear(in_features=640, out_features=640, bias=True)
                          (1): Dropout(p=0.0, inplace=False)
                        )
                        (processor): AttnProcessor2_0()
                      )
                      (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                      (attn2): Attention(
                        (to_q): Linear(in_features=640, out_features=640, bias=False)
                        (to_k): Linear(in_features=768, out_features=640, bias=False)
                        (to_v): Linear(in_features=768, out_features=640, bias=False)
                        (to_out): ModuleList(
                          (0): Linear(in_features=640, out_features=640, bias=True)
                          (1): Dropout(p=0.0, inplace=False)
                        )
                        (processor): SkipAttnProcessor()
                      )
                      (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                      (ff): FeedForward(
                        (net): ModuleList(
                          (0): GEGLU(
                            (proj): Linear(in_features=640, out_features=5120, bias=True)
                          )
                          (1): Dropout(p=0.0, inplace=False)
                          (2): Linear(in_features=2560, out_features=640, bias=True)
                        )
                      )
                    )
                  )
                  (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))
                )
              )
              (resnets): ModuleList(
                (0): ResnetBlock2D(
                  (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)
                  (conv1): Conv2d(1920, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)
                  (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)
                  (dropout): Dropout(p=0.0, inplace=False)
                  (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  (nonlinearity): SiLU()
                  (conv_shortcut): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))
                )
                (1): ResnetBlock2D(
                  (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)
                  (conv1): Conv2d(1280, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)
                  (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)
                  (dropout): Dropout(p=0.0, inplace=False)
                  (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  (nonlinearity): SiLU()
                  (conv_shortcut): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1))
                )
                (2): ResnetBlock2D(
                  (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)
                  (conv1): Conv2d(960, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)
                  (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)
                  (dropout): Dropout(p=0.0, inplace=False)
                  (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  (nonlinearity): SiLU()
                  (conv_shortcut): Conv2d(960, 640, kernel_size=(1, 1), stride=(1, 1))
                )
              )
              (upsamplers): ModuleList(
                (0): Upsample2D(
                  (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                )
              )
            )
            (3): CrossAttnUpBlock2D(
              (attentions): ModuleList(
                (0-2): 3 x Transformer2DModel(
                  (norm): GroupNorm(32, 320, eps=1e-06, affine=True)
                  (proj_in): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
                  (transformer_blocks): ModuleList(
                    (0): BasicTransformerBlock(
                      (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                      (attn1): Attention(
                        (to_q): Linear(in_features=320, out_features=320, bias=False)
                        (to_k): Linear(in_features=320, out_features=320, bias=False)
                        (to_v): Linear(in_features=320, out_features=320, bias=False)
                        (to_out): ModuleList(
                          (0): Linear(in_features=320, out_features=320, bias=True)
                          (1): Dropout(p=0.0, inplace=False)
                        )
                        (processor): AttnProcessor2_0()
                      )
                      (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                      (attn2): Attention(
                        (to_q): Linear(in_features=320, out_features=320, bias=False)
                        (to_k): Linear(in_features=768, out_features=320, bias=False)
                        (to_v): Linear(in_features=768, out_features=320, bias=False)
                        (to_out): ModuleList(
                          (0): Linear(in_features=320, out_features=320, bias=True)
                          (1): Dropout(p=0.0, inplace=False)
                        )
                        (processor): SkipAttnProcessor()
                      )
                      (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                      (ff): FeedForward(
                        (net): ModuleList(
                          (0): GEGLU(
                            (proj): Linear(in_features=320, out_features=2560, bias=True)
                          )
                          (1): Dropout(p=0.0, inplace=False)
                          (2): Linear(in_features=1280, out_features=320, bias=True)
                        )
                      )
                    )
                  )
                  (proj_out): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
                )
              )
              (resnets): ModuleList(
                (0): ResnetBlock2D(
                  (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)
                  (conv1): Conv2d(960, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)
                  (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)
                  (dropout): Dropout(p=0.0, inplace=False)
                  (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  (nonlinearity): SiLU()
                  (conv_shortcut): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1))
                )
                (1-2): 2 x ResnetBlock2D(
                  (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)
                  (conv1): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)
                  (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)
                  (dropout): Dropout(p=0.0, inplace=False)
                  (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  (nonlinearity): SiLU()
                  (conv_shortcut): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))
                )
              )
            )
          )
          (mid_block): UNetMidBlock2DCrossAttn(
            (attentions): ModuleList(
              (0): Transformer2DModel(
                (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
                (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))
                (transformer_blocks): ModuleList(
                  (0): BasicTransformerBlock(
                    (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                    (attn1): Attention(
                      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                      (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                      (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                      (to_out): ModuleList(
                        (0): Linear(in_features=1280, out_features=1280, bias=True)
                        (1): Dropout(p=0.0, inplace=False)
                      )
                      (processor): AttnProcessor2_0()
                    )
                    (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                    (attn2): Attention(
                      (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                      (to_k): Linear(in_features=768, out_features=1280, bias=False)
                      (to_v): Linear(in_features=768, out_features=1280, bias=False)
                      (to_out): ModuleList(
                        (0): Linear(in_features=1280, out_features=1280, bias=True)
                        (1): Dropout(p=0.0, inplace=False)
                      )
                      (processor): SkipAttnProcessor()
                    )
                    (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                    (ff): FeedForward(
                      (net): ModuleList(
                        (0): GEGLU(
                          (proj): Linear(in_features=1280, out_features=10240, bias=True)
                        )
                        (1): Dropout(p=0.0, inplace=False)
                        (2): Linear(in_features=5120, out_features=1280, bias=True)
                      )
                    )
                  )
                )
                (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))
              )
            )
            (resnets): ModuleList(
              (0-1): 2 x ResnetBlock2D(
                (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)
                (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)
                (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (nonlinearity): SiLU()
              )
            )
          )
          (conv_norm_out): GroupNorm(32, 320, eps=1e-05, affine=True)
          (conv_act): SiLU()
          (conv_out): Conv2d(320, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        ), learning_rate=0.0001)
train_batch_size:
    value: 6
unet:
    value: null
