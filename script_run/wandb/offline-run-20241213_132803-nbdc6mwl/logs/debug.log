2024-12-13 13:28:03,711 INFO    MainThread:3386426 [wandb_setup.py:_flush():79] Current SDK version is 0.18.7
2024-12-13 13:28:03,711 INFO    MainThread:3386426 [wandb_setup.py:_flush():79] Configure stats pid to 3386426
2024-12-13 13:28:03,711 INFO    MainThread:3386426 [wandb_setup.py:_flush():79] Loading settings from /home/tiennv/.config/wandb/settings
2024-12-13 13:28:03,711 INFO    MainThread:3386426 [wandb_setup.py:_flush():79] Loading settings from /mnt/Userdrive/tiennv/trang/chaos/trash/script_run/wandb/settings
2024-12-13 13:28:03,711 INFO    MainThread:3386426 [wandb_setup.py:_flush():79] Loading settings from environment variables: {}
2024-12-13 13:28:03,711 INFO    MainThread:3386426 [wandb_setup.py:_flush():79] Applying setup settings: {'mode': None, '_disable_service': None}
2024-12-13 13:28:03,711 INFO    MainThread:3386426 [wandb_setup.py:_flush():79] Inferring run settings from compute environment: {'program_relpath': 'train.py', 'program_abspath': '/mnt/Userdrive/tiennv/trang/chaos/trash/train.py', 'program': '/mnt/Userdrive/tiennv/trang/chaos/trash/script_run/../train.py'}
2024-12-13 13:28:03,711 INFO    MainThread:3386426 [wandb_init.py:_log_setup():533] Logging user logs to /mnt/Userdrive/tiennv/trang/chaos/trash/script_run/wandb/offline-run-20241213_132803-nbdc6mwl/logs/debug.log
2024-12-13 13:28:03,711 INFO    MainThread:3386426 [wandb_init.py:_log_setup():534] Logging internal logs to /mnt/Userdrive/tiennv/trang/chaos/trash/script_run/wandb/offline-run-20241213_132803-nbdc6mwl/logs/debug-internal.log
2024-12-13 13:28:03,711 INFO    MainThread:3386426 [wandb_init.py:init():619] calling init triggers
2024-12-13 13:28:03,711 INFO    MainThread:3386426 [wandb_init.py:init():626] wandb.init called with sweep_config: {}
config: {}
2024-12-13 13:28:03,711 INFO    MainThread:3386426 [wandb_init.py:init():669] starting backend
2024-12-13 13:28:03,711 INFO    MainThread:3386426 [wandb_init.py:init():673] sending inform_init request
2024-12-13 13:28:03,713 INFO    MainThread:3386426 [backend.py:_multiprocessing_setup():104] multiprocessing start_methods=fork,spawn,forkserver, using: spawn
2024-12-13 13:28:03,713 INFO    MainThread:3386426 [wandb_init.py:init():686] backend started and connected
2024-12-13 13:28:03,715 INFO    MainThread:3386426 [wandb_init.py:init():781] updated telemetry
2024-12-13 13:28:03,717 INFO    MainThread:3386426 [wandb_init.py:init():814] communicating run to backend with 90.0 second timeout
2024-12-13 13:28:03,835 INFO    MainThread:3386426 [wandb_init.py:init():867] starting run threads in backend
2024-12-13 13:28:03,982 INFO    MainThread:3386426 [wandb_run.py:_console_start():2456] atexit reg
2024-12-13 13:28:03,983 INFO    MainThread:3386426 [wandb_run.py:_redirect():2305] redirect: wrap_raw
2024-12-13 13:28:03,983 INFO    MainThread:3386426 [wandb_run.py:_redirect():2370] Wrapping output streams.
2024-12-13 13:28:03,983 INFO    MainThread:3386426 [wandb_run.py:_redirect():2395] Redirects installed.
2024-12-13 13:28:03,984 INFO    MainThread:3386426 [wandb_init.py:init():911] run started, returning control to user process
2024-12-13 13:28:03,984 INFO    MainThread:3386426 [wandb_run.py:_config_callback():1387] config_cb None None {'learning_rate': 5e-05, 'train_batch_size': 1, 'num_train_epochs': 5000, 'resolution': 512}
2024-12-13 13:28:13,435 INFO    MainThread:3386426 [wandb_run.py:_config_callback():1387] config_cb None None {'unet': None, 'device_train': 'cuda', 'noise_scheduler': 'DDIMScheduler {\n  "_class_name": "DDIMScheduler",\n  "_diffusers_version": "0.31.0",\n  "beta_end": 0.012,\n  "beta_schedule": "scaled_linear",\n  "beta_start": 0.00085,\n  "clip_sample": false,\n  "clip_sample_range": 1.0,\n  "dynamic_thresholding_ratio": 0.995,\n  "num_train_timesteps": 1000,\n  "prediction_type": "v_prediction",\n  "rescale_betas_zero_snr": false,\n  "sample_max_value": 1.0,\n  "set_alpha_to_one": false,\n  "skip_prk_steps": true,\n  "steps_offset": 1,\n  "thresholding": false,\n  "timestep_spacing": "leading",\n  "trained_betas": null\n}\n', 'scheduler_optimizer_config': 'TrainableParameters(unet=UNet2DConditionModel(\n  (conv_in): Conv2d(9, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (time_proj): Timesteps()\n  (time_embedding): TimestepEmbedding(\n    (linear_1): Linear(in_features=320, out_features=1280, bias=True)\n    (act): SiLU()\n    (linear_2): Linear(in_features=1280, out_features=1280, bias=True)\n  )\n  (down_blocks): ModuleList(\n    (0): CrossAttnDownBlock2D(\n      (attentions): ModuleList(\n        (0-1): 2 x Transformer2DModel(\n          (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n          (proj_in): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n          (transformer_blocks): ModuleList(\n            (0): BasicTransformerBlock(\n              (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n              (attn1): Attention(\n                (to_q): Linear(in_features=320, out_features=320, bias=False)\n                (to_k): Linear(in_features=320, out_features=320, bias=False)\n                (to_v): Linear(in_features=320, out_features=320, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=320, out_features=320, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n                (processor): AttnProcessor2_0()\n              )\n              (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n              (attn2): Attention(\n                (to_q): Linear(in_features=320, out_features=320, bias=False)\n                (to_k): Linear(in_features=768, out_features=320, bias=False)\n                (to_v): Linear(in_features=768, out_features=320, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=320, out_features=320, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n                (processor): SkipAttnProcessor()\n              )\n              (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n              (ff): FeedForward(\n                (net): ModuleList(\n                  (0): GEGLU(\n                    (proj): Linear(in_features=320, out_features=2560, bias=True)\n                  )\n                  (1): Dropout(p=0.0, inplace=False)\n                  (2): Linear(in_features=1280, out_features=320, bias=True)\n                )\n              )\n            )\n          )\n          (proj_out): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (resnets): ModuleList(\n        (0-1): 2 x ResnetBlock2D(\n          (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n          (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n        )\n      )\n      (downsamplers): ModuleList(\n        (0): Downsample2D(\n          (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        )\n      )\n    )\n    (1): CrossAttnDownBlock2D(\n      (attentions): ModuleList(\n        (0-1): 2 x Transformer2DModel(\n          (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n          (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n          (transformer_blocks): ModuleList(\n            (0): BasicTransformerBlock(\n              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n              (attn1): Attention(\n                (to_q): Linear(in_features=640, out_features=640, bias=False)\n                (to_k): Linear(in_features=640, out_features=640, bias=False)\n                (to_v): Linear(in_features=640, out_features=640, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=640, out_features=640, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n                (processor): AttnProcessor2_0()\n              )\n              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n              (attn2): Attention(\n                (to_q): Linear(in_features=640, out_features=640, bias=False)\n                (to_k): Linear(in_features=768, out_features=640, bias=False)\n                (to_v): Linear(in_features=768, out_features=640, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=640, out_features=640, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n                (processor): SkipAttnProcessor()\n              )\n              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n              (ff): FeedForward(\n                (net): ModuleList(\n                  (0): GEGLU(\n                    (proj): Linear(in_features=640, out_features=5120, bias=True)\n                  )\n                  (1): Dropout(p=0.0, inplace=False)\n                  (2): Linear(in_features=2560, out_features=640, bias=True)\n                )\n              )\n            )\n          )\n          (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (resnets): ModuleList(\n        (0): ResnetBlock2D(\n          (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n          (conv1): Conv2d(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): Conv2d(320, 640, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (1): ResnetBlock2D(\n          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n          (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n        )\n      )\n      (downsamplers): ModuleList(\n        (0): Downsample2D(\n          (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        )\n      )\n    )\n    (2): CrossAttnDownBlock2D(\n      (attentions): ModuleList(\n        (0-1): 2 x Transformer2DModel(\n          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n          (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n          (transformer_blocks): ModuleList(\n            (0): BasicTransformerBlock(\n              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n              (attn1): Attention(\n                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n                (processor): AttnProcessor2_0()\n              )\n              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n              (attn2): Attention(\n                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n                (to_k): Linear(in_features=768, out_features=1280, bias=False)\n                (to_v): Linear(in_features=768, out_features=1280, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n                (processor): SkipAttnProcessor()\n              )\n              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n              (ff): FeedForward(\n                (net): ModuleList(\n                  (0): GEGLU(\n                    (proj): Linear(in_features=1280, out_features=10240, bias=True)\n                  )\n                  (1): Dropout(p=0.0, inplace=False)\n                  (2): Linear(in_features=5120, out_features=1280, bias=True)\n                )\n              )\n            )\n          )\n          (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (resnets): ModuleList(\n        (0): ResnetBlock2D(\n          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n          (conv1): Conv2d(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): Conv2d(640, 1280, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (1): ResnetBlock2D(\n          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n        )\n      )\n      (downsamplers): ModuleList(\n        (0): Downsample2D(\n          (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        )\n      )\n    )\n    (3): DownBlock2D(\n      (resnets): ModuleList(\n        (0-1): 2 x ResnetBlock2D(\n          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n        )\n      )\n    )\n  )\n  (up_blocks): ModuleList(\n    (0): UpBlock2D(\n      (resnets): ModuleList(\n        (0-2): 3 x ResnetBlock2D(\n          (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n          (conv1): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (upsamplers): ModuleList(\n        (0): Upsample2D(\n          (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        )\n      )\n    )\n    (1): CrossAttnUpBlock2D(\n      (attentions): ModuleList(\n        (0-2): 3 x Transformer2DModel(\n          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n          (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n          (transformer_blocks): ModuleList(\n            (0): BasicTransformerBlock(\n              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n              (attn1): Attention(\n                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n                (processor): AttnProcessor2_0()\n              )\n              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n              (attn2): Attention(\n                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n                (to_k): Linear(in_features=768, out_features=1280, bias=False)\n                (to_v): Linear(in_features=768, out_features=1280, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n                (processor): SkipAttnProcessor()\n              )\n              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n              (ff): FeedForward(\n                (net): ModuleList(\n                  (0): GEGLU(\n                    (proj): Linear(in_features=1280, out_features=10240, bias=True)\n                  )\n                  (1): Dropout(p=0.0, inplace=False)\n                  (2): Linear(in_features=5120, out_features=1280, bias=True)\n                )\n              )\n            )\n          )\n          (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (resnets): ModuleList(\n        (0-1): 2 x ResnetBlock2D(\n          (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n          (conv1): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (2): ResnetBlock2D(\n          (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n          (conv1): Conv2d(1920, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): Conv2d(1920, 1280, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (upsamplers): ModuleList(\n        (0): Upsample2D(\n          (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        )\n      )\n    )\n    (2): CrossAttnUpBlock2D(\n      (attentions): ModuleList(\n        (0-2): 3 x Transformer2DModel(\n          (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n          (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n          (transformer_blocks): ModuleList(\n            (0): BasicTransformerBlock(\n              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n              (attn1): Attention(\n                (to_q): Linear(in_features=640, out_features=640, bias=False)\n                (to_k): Linear(in_features=640, out_features=640, bias=False)\n                (to_v): Linear(in_features=640, out_features=640, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=640, out_features=640, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n                (processor): AttnProcessor2_0()\n              )\n              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n              (attn2): Attention(\n                (to_q): Linear(in_features=640, out_features=640, bias=False)\n                (to_k): Linear(in_features=768, out_features=640, bias=False)\n                (to_v): Linear(in_features=768, out_features=640, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=640, out_features=640, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n                (processor): SkipAttnProcessor()\n              )\n              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n              (ff): FeedForward(\n                (net): ModuleList(\n                  (0): GEGLU(\n                    (proj): Linear(in_features=640, out_features=5120, bias=True)\n                  )\n                  (1): Dropout(p=0.0, inplace=False)\n                  (2): Linear(in_features=2560, out_features=640, bias=True)\n                )\n              )\n            )\n          )\n          (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (resnets): ModuleList(\n        (0): ResnetBlock2D(\n          (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n          (conv1): Conv2d(1920, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (1): ResnetBlock2D(\n          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n          (conv1): Conv2d(1280, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (2): ResnetBlock2D(\n          (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n          (conv1): Conv2d(960, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): Conv2d(960, 640, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (upsamplers): ModuleList(\n        (0): Upsample2D(\n          (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        )\n      )\n    )\n    (3): CrossAttnUpBlock2D(\n      (attentions): ModuleList(\n        (0-2): 3 x Transformer2DModel(\n          (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n          (proj_in): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n          (transformer_blocks): ModuleList(\n            (0): BasicTransformerBlock(\n              (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n              (attn1): Attention(\n                (to_q): Linear(in_features=320, out_features=320, bias=False)\n                (to_k): Linear(in_features=320, out_features=320, bias=False)\n                (to_v): Linear(in_features=320, out_features=320, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=320, out_features=320, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n                (processor): AttnProcessor2_0()\n              )\n              (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n              (attn2): Attention(\n                (to_q): Linear(in_features=320, out_features=320, bias=False)\n                (to_k): Linear(in_features=768, out_features=320, bias=False)\n                (to_v): Linear(in_features=768, out_features=320, bias=False)\n                (to_out): ModuleList(\n                  (0): Linear(in_features=320, out_features=320, bias=True)\n                  (1): Dropout(p=0.0, inplace=False)\n                )\n                (processor): SkipAttnProcessor()\n              )\n              (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n              (ff): FeedForward(\n                (net): ModuleList(\n                  (0): GEGLU(\n                    (proj): Linear(in_features=320, out_features=2560, bias=True)\n                  )\n                  (1): Dropout(p=0.0, inplace=False)\n                  (2): Linear(in_features=1280, out_features=320, bias=True)\n                )\n              )\n            )\n          )\n          (proj_out): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (resnets): ModuleList(\n        (0): ResnetBlock2D(\n          (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n          (conv1): Conv2d(960, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (1-2): 2 x ResnetBlock2D(\n          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n          (conv1): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (nonlinearity): SiLU()\n          (conv_shortcut): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n    )\n  )\n  (mid_block): UNetMidBlock2DCrossAttn(\n    (attentions): ModuleList(\n      (0): Transformer2DModel(\n        (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n        (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n        (transformer_blocks): ModuleList(\n          (0): BasicTransformerBlock(\n            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n            (attn1): Attention(\n              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n              (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n              (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n              (to_out): ModuleList(\n                (0): Linear(in_features=1280, out_features=1280, bias=True)\n                (1): Dropout(p=0.0, inplace=False)\n              )\n              (processor): AttnProcessor2_0()\n            )\n            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n            (attn2): Attention(\n              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n              (to_k): Linear(in_features=768, out_features=1280, bias=False)\n              (to_v): Linear(in_features=768, out_features=1280, bias=False)\n              (to_out): ModuleList(\n                (0): Linear(in_features=1280, out_features=1280, bias=True)\n                (1): Dropout(p=0.0, inplace=False)\n              )\n              (processor): SkipAttnProcessor()\n            )\n            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n            (ff): FeedForward(\n              (net): ModuleList(\n                (0): GEGLU(\n                  (proj): Linear(in_features=1280, out_features=10240, bias=True)\n                )\n                (1): Dropout(p=0.0, inplace=False)\n                (2): Linear(in_features=5120, out_features=1280, bias=True)\n              )\n            )\n          )\n        )\n        (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n      )\n    )\n    (resnets): ModuleList(\n      (0-1): 2 x ResnetBlock2D(\n        (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n        (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n        (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n        (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (nonlinearity): SiLU()\n      )\n    )\n  )\n  (conv_norm_out): GroupNorm(32, 320, eps=1e-05, affine=True)\n  (conv_act): SiLU()\n  (conv_out): Conv2d(320, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n), learning_rate=5e-05)', 'args': "Namespace(pretrained_model_name_or_path='botp/stable-diffusion-v1-5-inpainting', unet_model_name_or_path='/home/tiennv/trang/chaos/controlnext/weight_pretrain/unet_inpainting', tokenizer_name=None, load_unet_increaments=None, dataset_path='/home/tiennv/trang/chaos/controlnext/data/datatest/data_high_quality.csv', save_embeddings_to_npz=True, revision=None, variant=None, output_dir='/home/tiennv/trang/chaos/trash/output', seed=None, resolution=512, train_batch_size=1, num_train_epochs=5000, max_train_steps=None, checkpointing_steps=500, checkpoints_total_limit=5, resume_from_checkpoint='latest', input_type='raw', gradient_accumulation_steps=1, gradient_checkpointing=False, learning_rate=5e-05, tensor_dtype_save='fp16', path_to_save_data_embedding='/home/tiennv/trang/chaos/controlnext/data/datatest/dataset_few', scale_lr=False, lr_scheduler='constant', prediction_type='v_prediction', lr_warmup_steps=500, lr_num_cycles=1, lr_power=1.0, dataloader_num_workers=0, adam_beta1=0.9, adam_beta2=0.999, adam_weight_decay=1e-06, adam_epsilon=1e-08, max_grad_norm=1.0, logging_dir='logs', allow_tf32=False, report_to='wandb', mixed_precision='bf16', enable_xformers_memory_efficient_attention=False, set_grads_to_none=False, max_train_samples=None, proportion_empty_prompts=0, tracker_project_name='train_controlnext')"}
